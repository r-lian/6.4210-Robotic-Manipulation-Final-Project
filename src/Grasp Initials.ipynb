{"cells":[{"cell_type":"markdown","metadata":{"id":"EgiF12Hf1Dhs","colab_type":"text","cell_id":"60a219dc6d404e91bf260ff1e516bd70","deepnote_cell_type":"markdown"},"source":"# Sampling Grasp Meshes\n\nIn previous chapters, we manually specified grasps for the objects our robot was manipulating. This is fine when you are only manipulating a fixed object in a fixed scene, but we want our robots to be able to manipulate all sorts of objects in scenes we have not seen before. In this notebook, we will build familiarity with methods for generating grasp poses from arbitary meshes. \n\n\n**Learning Objectives:**\n1. Antipodal grasp sampling on a mesh\n2. Heuristic design for grasp filtering\n\n**What you'll build:** A simulation of the IIWA grasping and reorienting meshes corresponding to your initials. \n\n**Reference:** Make sure you understand the full grasp sampling demo in [Chapter 5](https://manipulation.mit.edu/clutter.html#grasp_sampling), many of the same principles apply. It will also be helpful, but not necessary, to have solved [Exercise 4.11](https://manipulation.mit.edu/pose.html#exercises).\n\nYour end result will look something like this, where the letters are spawned laying down on the table, and the iiwa will pick them up from above:\n\n![geometry_pick_and_place_point_clouds.png](https://raw.githubusercontent.com/RussTedrake/manipulation/master/book/figures/exercises/clutter_sampling_grasps_letters.png\n)\n\nLet's start by getting our imports out of the way and launching Meshcat. ","block_group":"f503c61bbe904ad4b91e64fabf0e2ab9"},{"cell_type":"code","metadata":{"id":"eeMrMI0-1Dhu","colab":{},"colab_type":"code","source_hash":"a6bbe906","execution_start":1759958283426,"execution_millis":2948,"execution_context_id":"dd770d44-509b-4071-9ebc-058967a5ad4a","cell_id":"7e8cfa0f67894985b000c8a9b4bf8cb6","deepnote_cell_type":"code"},"source":"import os\nimport random\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nimport mpld3\nimport numpy as np\nimport trimesh\nfrom pydrake.all import (\n    AddFrameTriadIllustration,\n    BasicVector,\n    Context,\n    DiagramBuilder,\n    Integrator,\n    JacobianWrtVariable,\n    LeafSystem,\n    ModelInstanceIndex,\n    MultibodyPlant,\n    Parser,\n    PiecewisePolynomial,\n    PiecewisePose,\n    RigidTransform,\n    RotationMatrix,\n    Simulator,\n    StartMeshcat,\n    TrajectorySource,\n)\n\nfrom manipulation import running_as_notebook\nfrom manipulation.exercises.clutter.test_grasp_letters import TestLetterGrasp\nfrom manipulation.exercises.grader import Grader\nfrom manipulation.letter_generation import create_sdf_asset_from_letter\nfrom manipulation.station import LoadScenario, MakeHardwareStation\n\nif running_as_notebook:\n    mpld3.enable_notebook()\n\n# Start the visualizer.\nmeshcat = StartMeshcat()","block_group":"2e0b558309b04861ad6b5b00f7d5851d","execution_count":1,"outputs":[{"name":"stderr","text":"INFO:drake:Meshcat listening for connections at https://a12a817b-ea1f-4d7a-a3ea-c55a125b69f8.deepnoteproject.com/7000/\nInstalling NginX server for MeshCat on Deepnote...\n","output_type":"stream"},{"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Meshcat URL: <a href='https://a12a817b-ea1f-4d7a-a3ea-c55a125b69f8.deepnoteproject.com/7000/' target='_blank'>https://a12a817b-ea1f-4d7a-a3ea-c55a125b69f8.deepnoteproject.com/7000/</a>"},"metadata":{},"output_type":"display_data"}],"outputs_reference":"dbtable:cell_outputs/049bb59d-3375-4291-aec3-106d1a0aaf0c","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"18712e0d25a24597b7cf259f10ef887c","deepnote_cell_type":"markdown"},"source":"# Mesh Pre-Processing\n\nThe first step will be load in the geometry of the part we are manipulating. Because the focus of this excercise is on grasp sampling, we will assume access to the ground truth pose of the part on the table and its geometry.","block_group":"a71224de37fd4ffab6053573599c2e98"},{"cell_type":"code","metadata":{"source_hash":"ce55d98f","execution_start":1759958286426,"execution_millis":1,"execution_context_id":"dd770d44-509b-4071-9ebc-058967a5ad4a","cell_id":"507ce917606f4d868ba0789ad4b7d7e9","deepnote_cell_type":"code"},"source":"# TODO fill in your initials here.\ninitials = \"AB\"","block_group":"36b8ac74e2da4dd0b03e361866e13da1","execution_count":2,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"4711793f","execution_start":1759958286476,"execution_millis":12593,"execution_context_id":"dd770d44-509b-4071-9ebc-058967a5ad4a","cell_id":"8774cbd2a59e402abddb8ce4275c5954","deepnote_cell_type":"code"},"source":"create_sdf_asset_from_letter(\n    text=initials[0],\n    font_name=\"DejaVu Sans\",\n    letter_height_meters=0.25,\n    extrusion_depth_meters=0.07,\n    output_dir=\"assets\",\n    include_normals=True,\n    mu_static=1.17,\n    mass=0.1,\n)\ncreate_sdf_asset_from_letter(\n    text=initials[1],\n    font_name=\"DejaVu Sans\",\n    letter_height_meters=0.25,\n    extrusion_depth_meters=0.07,\n    output_dir=\"assets\",\n    include_normals=True,\n    mu_static=1.17,\n    mass=0.1,\n)","block_group":"bdc4c44a4d864f3280971cba29fc7a66","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"PosixPath('assets/B.sdf')"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/17a61845-3a41-407b-91eb-3a2670546555","content_dependencies":null},{"cell_type":"code","metadata":{"scrolled":true,"source_hash":"94e1a39d","code_folding":[],"execution_start":1759958299126,"execution_millis":1,"execution_context_id":"dd770d44-509b-4071-9ebc-058967a5ad4a","cell_id":"fd17a0cc6c324a5f9354e1a54eea1241","deepnote_cell_type":"code"},"source":"# TODO: load your first initial with trimesh.load(...) as a mesh.\n# To do this, you should make sure to use the kwargs force=\"mesh\".\n# See the docs for more info at https://trimesh.org/. (see exercise 4.1)\n\n\ndef load_first_initial() -> trimesh.Trimesh:\n    return trimesh.load(f\"assets/{initials[0]}.obj\", force=\"mesh\")","block_group":"d8e111c0cc37436a93c7dcf3155c40a0","execution_count":4,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"dd94b802d4044b8da00f16e87ae90fde","deepnote_cell_type":"markdown"},"source":"# Grasp Sampling\n\nThe next task will be to find candidate grasps. We are looking for collinear, antipodal points that can fit within the width of the gripper and do not put the gripper in collision. From these points, we can define gripper poses that we command the robot to achieve. We will break this into three steps. \n\n1. Finding Pairs of Collinear Points via ray casting\n2. Taking a pair of colinear points and using them to compute a gripper pose\n2. Filtering grasps on antipodality, finger width, and collision\n\n**Reference:** You will need to call the following functions from `trimesh` when sampling colinear points:\n\n- [sample_surface](https://trimesh.org/trimesh.sample.html#trimesh.sample.sample_surface)\n\n- [intersects_location](https://trimesh.org/trimesh.ray.ray_pyembree.html#trimesh.ray.ray_pyembree.RayMeshIntersector.intersects_location)\n\nAnd constructing the grasp transform from a point and its normal is demonstrated in the demo from [Example 5.12 in the textbook](https://manipulation.mit.edu/clutter.html#grasp_sampling).","block_group":"d07a12b360614884a9f0d3fc98e87f0f"},{"cell_type":"code","metadata":{"source_hash":"9464adce","code_folding":[],"execution_start":1759958299186,"execution_millis":0,"execution_context_id":"dd770d44-509b-4071-9ebc-058967a5ad4a","cell_id":"5381527b581c47f39f053076b6e79c65","deepnote_cell_type":"code"},"source":"# Structure: (point_1, point_2, normal_1, normal_2)\nAntipodeCandidateType = Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n\n\ndef sample_colinear_points(\n    mesh: trimesh.Trimesh, n_sample_points: int\n) -> List[AntipodeCandidateType]:\n    \"\"\"\n    Compute n_sample_points point pairs for the mesh that are colinear.\n    This is done by sampling points from the surface and casting a ray along the normal vector of that point\n    until another point on the mesh surface is hit. This function returns\n    a list of length n_sample_points of tuples, with every tuple having the structure:\n    (point_1, point_2, normal_1, normal_2)\n\n    Note that the returned points will be expressed in the object frame.\n    \"\"\"\n    candidates = []\n\n    # TODO: sample `n_sample_points` from the surface of the mesh\n    samples, face_index = trimesh.sample.sample_surface(mesh, n_sample_points)\n\n    # TODO: index into the mesh `face_normals` to get the normals q for all the sampled points\n    qs = [mesh.face_normals[face_index[i]] for i in range(n_sample_points)]\n\n    for i in range(n_sample_points):\n        # TODO: check for the first point hit while traversing a ray starting at the sampled point\n        # and moving along the negative of the normal vector\n        origin = samples[i]\n        direction = -qs[i]\n        locations, index_ray, index_tri = mesh.ray.intersects_location([origin], [direction])\n\n        # TODO: if no hits are found, skip this point\n        if len(locations) == 0:\n            continue\n\n        # TODO: get the `face_normal` from the mesh corresponding to the colinear point.\n        face_normal = mesh.face_normals[index_tri[0]]\n\n        # TODO: add the tuple of the two points and their normals to the `candidates` list\n        candidates.append((origin, locations[0], qs[i], face_normal))\n    return candidates","block_group":"7490d4067a5f45f8909aed2395741ca5","execution_count":5,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"76cbeb21","execution_start":1759958299236,"execution_millis":1,"execution_context_id":"dd770d44-509b-4071-9ebc-058967a5ad4a","cell_id":"5acd7e47c4dd40e696f18702a265e3f6","deepnote_cell_type":"code"},"source":"def compute_grasp_from_points(\n    antipodal_pt: AntipodeCandidateType,\n) -> RigidTransform | None:\n    \"\"\"\n    Given the tuple of antipodal points and their normals on the object O, compute the grasp X_OG.\n    \"\"\"\n    z_axis_O = np.array([0.0, 0.0, 1.0])\n\n    # TODO: the x-axis of frame G is given by the normal of the sampled point in frame O\n    x_axis_G = antipodal_pt[2] / np.linalg.norm(antipodal_pt[2])\n\n    # TODO: if the x-axis of frame G is parallel to the frame O z-axis, return None\n    if np.allclose(np.cross(x_axis_G, z_axis_O), 0):\n        return None\n\n    # TODO: the y-axis of frame G should point along the -z axis of the O, such\n    #       that we pick up the object from above.\n    #       The x and y axis have to be orthogonal, so project. (Hint: Gram-Schmidt)\n    y_axis_G = z_axis_O - np.dot(z_axis_O, x_axis_G) * x_axis_G\n    y_axis_G /= np.linalg.norm(y_axis_G)\n    if np.dot(y_axis_G, -z_axis_O) < 0:\n        y_axis_G = -y_axis_G\n\n    # TODO: the z-axis of frame G is orthogonal to the x- and y-axis of frame G\n    z_axis_G = np.cross(x_axis_G, y_axis_G)\n    z_axis_G /= np.linalg.norm(z_axis_G)\n\n    # TODO: construct the rotation matrix R_OG\n    R_OG = RotationMatrix(np.column_stack((x_axis_G, y_axis_G, z_axis_G)))\n\n    # TODO: define p_OG_O by computing the median of the two colinear points.\n    p_OG_O = 0.5 * (antipodal_pt[0] + antipodal_pt[1])\n\n    # TODO: define the transform X_OG, then add an offset of -0.1m in the y-axis to account for finger length\n    # return the resulting transform\n    X_OG = RigidTransform(R_OG, p_OG_O)\n    X_OG = X_OG @ RigidTransform([0, -0.1, 0])\n    return X_OG","block_group":"c45a4e14874c49d3bdc74badc612d051","execution_count":6,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"3e1dd5b4","code_folding":[],"execution_start":1759958299296,"execution_millis":1,"execution_context_id":"dd770d44-509b-4071-9ebc-058967a5ad4a","cell_id":"966646b105ec4684b6b1268190c84d09","deepnote_cell_type":"code"},"source":"def check_collision_free(X_WG: RigidTransform) -> bool:\n    \"\"\"\n    Checks if the gripper collides with the table. The table can be represented as a flat plane centered at the\n    origin with a normal vector pointing in the z-axis in world coordinates.\n    params:\n        X_WG (ndarray): (4x4) matrix describing gripper frame in the world coordinates\n    returns:\n        True if the gripper is in a collision free pose, False otherwise\n    \"\"\"\n    gripper_vertices = np.array([[-0.073, -0.085383, -0.025], [0.073, 0.069, 0.025]])\n    # vertices modeling the gripper collision body in homogenous coordinates\n    verts_h = np.hstack((gripper_vertices, np.ones((gripper_vertices.shape[0], 1))))\n\n    # TODO: map the gripper vertices to the world frame\n    X_WG_h = np.eye(4)\n    X_WG_h[:3, :3] = X_WG.rotation().matrix()\n    X_WG_h[:3, 3]  = X_WG.translation()\n    verts_W = X_WG_h @ verts_h.T\n\n    # TODO: the gripper is collision free if all the vertices are above z=0.\n    if np.all(verts_W[2, :] > 0):\n        return True\n    return False\n    # return true if collision free, false otherwise","block_group":"50957d64d2f94b7faff70bc232463f90","execution_count":7,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"ce7b458b","execution_start":1759958299346,"execution_millis":1,"execution_context_id":"dd770d44-509b-4071-9ebc-058967a5ad4a","cell_id":"942e5efb9ccb4f18a69efade88f2bd06","deepnote_cell_type":"code"},"source":"def get_filtered_grasps(\n    candidate_list: List[AntipodeCandidateType],\n    antipodal_thresh: float,\n    z_axis_thresh: float,\n    max_pt_dist: float,\n    min_pt_dist: float,\n    X_WO: RigidTransform,\n) -> List[RigidTransform]:\n    \"\"\"\n    Return a list of grasps filtered on the following criteria\n    (1) Antipodality: antipodality is a good heuristic for finding grasps with a large total wrench cone\n    (2) Point Distance: pairs of points too far apart won't fit inside the gripper.\n        Points too close together are \"false positives\", that appear due to the numerics of the ray casting.\n    (3) Collision:\n    \"\"\"\n    filtered_candidates = []\n    for candidate in candidate_list:\n        # TODO: compute the dot product of the normals.\n        # If the points are roughly antipodal, their dot product will be less than the antipodal_thresh.\n        if np.dot(candidate[2], candidate[3]) > antipodal_thresh:\n            continue\n\n        # TODO: compute the distance between the point pairs. check that it is between\n        # max_pt_dist and min_pt_dist.\n        if np.linalg.norm(candidate[0] - candidate[1]) > max_pt_dist:\n            continue\n        if np.linalg.norm(candidate[0] - candidate[1]) < min_pt_dist:\n            continue\n\n        # TODO: compute the grasp corresponding to the candidate list\n        # if the grasp computation fails (returns None), then `continue` to the next candidate\n        X_OG = compute_grasp_from_points(candidate)\n        if X_OG is None:\n            continue\n\n        # TODO: map the grasp to the world frame and check if it is collision free.\n        X_WG = X_WO @ X_OG\n        if not check_collision_free(X_WG):\n            continue\n\n        # TODO: If a candidate passes all three checks, add the grasp in world-frame to `filtered_candidates`\n        filtered_candidates.append(X_WG)\n    return filtered_candidates","block_group":"a320c08026fb4f10b8f566950dc7a8b8","execution_count":8,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"d3e85301","execution_start":1759958299406,"execution_millis":0,"execution_context_id":"dd770d44-509b-4071-9ebc-058967a5ad4a","cell_id":"691e13cea6a944ffa7e10599cec77f78","deepnote_cell_type":"code"},"source":"def sample_grasp(\n    mesh: trimesh.Trimesh, X_WO: RigidTransform, n_sample_pts: int = 500\n) -> RigidTransform:\n    colinear_pts = sample_colinear_points(mesh, n_sample_points=n_sample_pts)\n    candidate_grasps = get_filtered_grasps(\n        colinear_pts,\n        antipodal_thresh=-0.95,\n        z_axis_thresh=0.8,\n        max_pt_dist=0.04,\n        min_pt_dist=0.005,\n        X_WO=X_WO,\n    )\n    return candidate_grasps[0]","block_group":"c3832563254045a3a97c974965044228","execution_count":9,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"da54b19b","execution_start":1759958299456,"execution_millis":0,"execution_context_id":"dd770d44-509b-4071-9ebc-058967a5ad4a","cell_id":"bae83e81412a4955bd9a6140955c1145","deepnote_cell_type":"code"},"source":"def compute_prepick_pose(X_WG: RigidTransform) -> RigidTransform:\n    X_GGprepick = RigidTransform([0, -0.17, 0.0])\n    return X_WG @ X_GGprepick","block_group":"f20fd00445624538bfb1cf271f063391","execution_count":10,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"c344a199b291478b8fc5ff25bf48c286","deepnote_cell_type":"markdown"},"source":"# Building the Diagram\n\nThe next few steps should look familiar. We will define a jacobian pseudo-inverse based controller, and define a yaml with all the geometries in our scene. The last step will be to use the grasps to define a robot trajectory.\n\n**Fill out the keyframes so that the robot starts at the initial pose. Then it:**\n\n**(1) goes to a randomly sampled grasp.**\n\n**(2) rotates the letter 30 degrees clockwise about the gripper y-axis.**\n\nWhen the robot is done, the first and second initial should have the same orientation.\nIf you find that the robot's fingers are bumping into the letter on its' way to manipulate it, try adjusting the `opened` constant, which controls the finger width when the robot is not grasping something. It may be helpful to go to a pre-pick pose before step 1 and step 2.","block_group":"e000f14c6bfe4ec7bbc7e7750a178484"},{"cell_type":"code","metadata":{"source_hash":"108639a7","execution_start":1759958299506,"execution_millis":0,"execution_context_id":"dd770d44-509b-4071-9ebc-058967a5ad4a","cell_id":"6956b73e31034e20917dde42992eddec","deepnote_cell_type":"code"},"source":"class PseudoInverseController(LeafSystem):\n    def __init__(self, plant: MultibodyPlant) -> None:\n        LeafSystem.__init__(self)\n        self._plant = plant\n        self._plant_context = plant.CreateDefaultContext()\n        self._iiwa = plant.GetModelInstanceByName(\"iiwa\")\n        self._G = plant.GetBodyByName(\"body\").body_frame()\n        self._W = plant.world_frame()\n\n        self.V_G_port = self.DeclareVectorInputPort(\"V_WG\", 6)\n        self.q_port = self.DeclareVectorInputPort(\"iiwa.position\", 7)\n        self.DeclareVectorOutputPort(\"iiwa.velocity\", 7, self.CalcOutput)\n        self.iiwa_start = plant.GetJointByName(\"iiwa_joint_1\").velocity_start()\n        self.iiwa_end = plant.GetJointByName(\"iiwa_joint_7\").velocity_start()\n\n    def CalcOutput(self, context: Context, output: BasicVector) -> None:\n        V_G = self.V_G_port.Eval(context)\n        q = self.q_port.Eval(context)\n        self._plant.SetPositions(self._plant_context, self._iiwa, q)\n        J_G = self._plant.CalcJacobianSpatialVelocity(\n            self._plant_context,\n            JacobianWrtVariable.kV,\n            self._G,\n            [0, 0, 0],\n            self._W,\n            self._W,\n        )\n        J_G = J_G[:, self.iiwa_start : self.iiwa_end + 1]  # Only iiwa terms.\n        v = np.linalg.pinv(J_G).dot(V_G)\n        output.SetFromVector(v)","block_group":"23980e94a4f848e9918df22143d73fef","execution_count":11,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"7b20b1d8","code_folding":[],"execution_start":1759958299556,"execution_millis":209,"execution_context_id":"dd770d44-509b-4071-9ebc-058967a5ad4a","cell_id":"69caab9847a74eb591139e4023801bc9","deepnote_cell_type":"code"},"source":"scenario_yaml = f\"\"\"directives:\n    - add_model:\n        name: iiwa\n        file: package://drake_models/iiwa_description/sdf/iiwa7_no_collision.sdf\n        default_joint_positions:\n            iiwa_joint_1: [-1.57]\n            iiwa_joint_2: [0.1]\n            iiwa_joint_3: [0]\n            iiwa_joint_4: [-1.2]\n            iiwa_joint_5: [0]\n            iiwa_joint_6: [ 1.6]\n            iiwa_joint_7: [0]\n    - add_weld:\n        parent: world\n        child: iiwa::iiwa_link_0\n        X_PC:\n            translation: [0, -0.5, 0]\n            rotation: !Rpy {{ deg: [0, 0, 180] }}\n    - add_model:\n        name: wsg\n        file: package://manipulation/hydro/schunk_wsg_50_with_tip.sdf\n    - add_weld:\n        parent: iiwa::iiwa_link_7\n        child: wsg::body\n        X_PC:\n            translation: [0, 0, 0.09]\n            rotation: !Rpy {{deg: [90, 0, 90]}}\n    - add_model:\n        name: table\n        file: package://manipulation/table.sdf\n    - add_weld:\n        parent: world\n        child: table::table_link\n        X_PC:\n            translation: [0.0, 0.0, -0.05]\n            rotation: !Rpy {{ deg: [0, 0, -90] }}\n    - add_model:\n        name: {initials[0]}_letter\n        file: file://{Path.cwd()}/assets/{initials[0]}.sdf\n        default_free_body_pose:\n            {initials[0]}_body_link:\n                translation: [-0.2, 0, 0]\n                rotation: !Rpy {{ deg: [0, 0, 30] }}\n    - add_model:\n        name: {initials[1]}_letter\n        file: file://{Path.cwd()}/assets/{initials[1]}.sdf\n        default_free_body_pose:\n            {initials[1]}_body_link:\n                translation: [0.25, 0, 0]\n                rotation: !Rpy {{ deg: [0, 0, 0] }}\nmodel_drivers:\n    iiwa: !IiwaDriver\n        control_mode: position_only\n        hand_model_name: wsg\n    wsg: !SchunkWsgDriver {{}}\n\"\"\"\nwith open(\"scene.yaml\", \"w\") as f:\n    f.write(scenario_yaml)","block_group":"0cdb322ac9d0416bb00e435e77b77c4c","execution_count":12,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"9d56a50b","code_folding":[],"execution_start":1759958299826,"execution_millis":7125,"execution_context_id":"dd770d44-509b-4071-9ebc-058967a5ad4a","cell_id":"d3c7a4efa75e4481bd780cef96552a6b","deepnote_cell_type":"code"},"source":"station = MakeHardwareStation(LoadScenario(filename=\"scene.yaml\"), meshcat=meshcat)\nbuilder = DiagramBuilder()\nbuilder.AddSystem(station)\n\nplant = station.GetSubsystemByName(\"plant\")\ntemp_context = station.CreateDefaultContext()\ntemp_plant_context = plant.GetMyContextFromRoot(temp_context)\nX_WGinitial = plant.EvalBodyPoseInWorld(temp_plant_context, plant.GetBodyByName(\"body\"))\n\nmodel_instance0 = plant.GetModelInstanceByName(f\"{initials[0]}_letter\")\nmodel_instance1 = plant.GetModelInstanceByName(f\"{initials[1]}_letter\")\nX_WO1initial = plant.EvalBodyPoseInWorld(\n    temp_plant_context, plant.GetBodyByName(f\"{initials[0]}_body_link\", model_instance0)\n)\n\nopened = 0.04\nclosed = 0.0\n\n# TODO: redefine `keyframes` so the robot performs the behavior described above.\n# `keyframes` is a list of 2-tuples. The first element in each tuple is a\n# gripper pose in the world frame. the second element is a float corresponding to the wsg position.\n# you can use `sample_grasp` to get a grasp pose.\n# a helper function to go to prepick poses has been provided (`compute_prepick_pose`).\ngrasp_pose = sample_grasp(load_first_initial(), X_WO1initial)\ngrasp_pose_final = grasp_pose @ RigidTransform(RotationMatrix.MakeYRotation(np.pi / 6), [0, 0, 0])\nkeyframes = [(X_WGinitial, opened), (compute_prepick_pose(grasp_pose), opened), (grasp_pose, opened), (grasp_pose, closed), (grasp_pose_final, closed)]\n\nsample_times = [3 * i for i in range(len(keyframes))]\nrobot_position_trajectory = PiecewisePose.MakeLinear(\n    sample_times, [kf[0] for kf in keyframes]\n)\ntraj_V_G = robot_position_trajectory.MakeDerivative()\ngripper_values = np.array([kf[1] for kf in keyframes])[None]\ntraj_wsg_command = PiecewisePolynomial.FirstOrderHold(sample_times, gripper_values)\nV_G_source = builder.AddSystem(TrajectorySource(traj_V_G))\ncontroller = builder.AddSystem(PseudoInverseController(plant))\nintegrator = builder.AddSystem(Integrator(7))\nwsg_source = builder.AddSystem(TrajectorySource(traj_wsg_command))\n\nbuilder.Connect(V_G_source.get_output_port(), controller.GetInputPort(\"V_WG\"))\nbuilder.Connect(controller.get_output_port(), integrator.get_input_port())\nbuilder.Connect(integrator.get_output_port(), station.GetInputPort(\"iiwa.position\"))\nbuilder.Connect(\n    station.GetOutputPort(\"iiwa.position_measured\"),\n    controller.GetInputPort(\"iiwa.position\"),\n)\n\n# visualize axes (useful for debugging)\nscenegraph = station.GetSubsystemByName(\"scene_graph\")\nAddFrameTriadIllustration(\n    scene_graph=scenegraph,\n    body=plant.GetBodyByName(f\"{initials[0]}_body_link\", model_instance0),\n    length=0.1,\n)\nAddFrameTriadIllustration(\n    scene_graph=scenegraph,\n    body=plant.GetBodyByName(f\"{initials[1]}_body_link\", model_instance1),\n    length=0.1,\n)\nAddFrameTriadIllustration(\n    scene_graph=scenegraph, body=plant.GetBodyByName(\"body\"), length=0.1\n)\n\nbuilder.Connect(wsg_source.get_output_port(), station.GetInputPort(\"wsg.position\"))\ndiagram = builder.Build()","block_group":"9e85fa59c5414eaf86e2b9fdd0c484ce","execution_count":13,"outputs":[{"name":"stderr","text":"INFO:drake:PackageMap: Downloading https://github.com/RobotLocomotion/models/archive/7b92aacbe021861ec9bbbb82d8ab9a19ded970ff.tar.gz\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/be46ba90-363c-404c-83d0-ef6f17053bb4","content_dependencies":null},{"cell_type":"code","metadata":{"scrolled":true,"source_hash":"5dd2c44e","execution_start":1759958307196,"execution_millis":16050,"execution_context_id":"dd770d44-509b-4071-9ebc-058967a5ad4a","cell_id":"551227ed86d84bdda88a3ac2c69f630f","deepnote_cell_type":"code"},"source":"# Define the simulator.\nsimulator = Simulator(diagram)\ncontext = simulator.get_mutable_context()\nstation_context = station.GetMyContextFromRoot(context)\nintegrator.set_integral_value(\n    integrator.GetMyContextFromRoot(context),\n    plant.GetPositions(\n        plant.GetMyContextFromRoot(context),\n        plant.GetModelInstanceByName(\"iiwa\"),\n    ),\n)\ndiagram.ForcedPublish(context)\nprint(f\"sanity check, simulation will run for {traj_V_G.end_time()} seconds\")\n\n# run simulation!\nmeshcat.StartRecording()\nif running_as_notebook:\n    simulator.set_target_realtime_rate(1.0)\nsimulator.AdvanceTo(traj_V_G.end_time())\nmeshcat.StopRecording()\nmeshcat.PublishRecording()","block_group":"74b5d493bcfe406d8e81df61057496a1","execution_count":14,"outputs":[{"name":"stdout","text":"sanity check, simulation will run for 12.0 seconds\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/fc91abb7-2710-4190-a70b-490008f0df4e","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"94eb13eac91f4f88a1355b0ad94d8d83","deepnote_cell_type":"markdown"},"source":"# Gradescope Verification\n\nTake a video of the trajectory and upload it to gradescope as an mp4, the file should be (much) smaller than 500MB. The robot should grasp the first initial using an antipodal grasp and rotate is so it has the same orientation as the second initial. Optionally, consider adding more advanced heuristics, like checking for collision between the gripper and the mesh in the pick pose as is done in chapter 5. ","block_group":"78611c33aa114b62ad0b49472a9662b1"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a12a817b-ea1f-4d7a-a3ea-c55a125b69f8' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"bba72196770046a4a34dc3f8da688432"}}